{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c7d0cf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1213404070.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [32], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    with open (\"C:/Users/VaishuSistas/Desktop/NLP/Assignment2/sts2016-english-with-gs-v1.0/STS2016.input.plagiarism.txt\", 'rt',encoding=\"utf8\") as myfile:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "texts = []\n",
    "#with open (\"C:/Users/VaishuSistas/Desktop/NLP/Assignment2/sts2016-english-with-gs-v1.0/STS2016.input.answer-answer.txt\", 'rt',encoding=\"utf8\") as myfile:  \n",
    "#with open (\"C:/Users/VaishuSistas/Desktop/NLP/Assignment2/sts2016-english-with-gs-v1.0/STS2016.input.headlines.txt\", 'rt',encoding=\"utf8\") as myfile:  \n",
    "#with open (\"C:/Users/VaishuSistas/Desktop/NLP/Assignment2/sts2016-english-with-gs-v1.0/STS2016.input.postediting.txt\", 'rt',encoding=\"utf8\") as myfile:  \n",
    "#with open (\"C:/Users/VaishuSistas/Desktop/NLP/Assignment2/sts2016-english-with-gs-v1.0/STS2016.input.question-question.txt\", 'rt',encoding=\"utf8\") as myfile:  \n",
    "\n",
    "\n",
    " with open (\"C:/Users/VaishuSistas/Desktop/NLP/Assignment2/sts2016-english-with-gs-v1.0/STS2016.input.plagiarism.txt\", 'rt',encoding=\"utf8\") as myfile:  \n",
    "\n",
    "    for myline in myfile:\n",
    "        texts.append(myline)\n",
    "\n",
    "\n",
    "\n",
    "sentence_list = [x for i in texts for x in i.split(\"\\t\")]\n",
    "\n",
    "out = [sentence_list[i: i+4] for i in range(0, len(sentence_list), 4)]\n",
    "\n",
    "df_inferSent = pd.DataFrame(out, columns = ['Sentence 1', 'Sentence 2', 'Link 1', 'Link 2'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3918a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-nli-mean-tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d78c4555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.23.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.10.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.13.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.9.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: numpy in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.23.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.1.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: filelock in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision->sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vaishusistas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ea0f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bf58d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0632843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_2 = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20a45632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e51fd110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "# Getting the columns containing the sentences into a list\n",
    "\n",
    "SentencesSbert_1 = df_inferSent[\"Sentence 1\"].tolist()\n",
    "\n",
    "SentencesSbert_2 = df_inferSent[\"Sentence 2\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Computing the embeddings for the sentences\n",
    "\n",
    "EmbeddingsSbert_1 = model.encode(SentencesSbert_1, convert_to_tensor=True)\n",
    "\n",
    "EmbeddingsSbert_2 = model.encode(SentencesSbert_2, convert_to_tensor=True)\n",
    "\n",
    "\n",
    "\n",
    "# Calculting the cosine similarity of each sentences' pair, and multiple the similarity score by 5\n",
    "\n",
    "cosine_scores_sbert_sentences = util.cos_sim(EmbeddingsSbert_1, EmbeddingsSbert_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64cf450f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file encoder already exists.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  146M    0  377k    0     0   311k      0  0:08:02  0:00:01  0:08:01  315k\n",
      "  1  146M    1 2369k    0     0  1101k      0  0:02:16  0:00:02  0:02:14 1108k\n",
      " 12  146M   12 19.0M    0     0  6192k      0  0:00:24  0:00:03  0:00:21 6217k\n",
      " 24  146M   24 35.6M    0     0  8796k      0  0:00:17  0:00:04  0:00:13 8824k\n",
      " 35  146M   35 52.6M    0     0  10.2M      0  0:00:14  0:00:05  0:00:09 10.5M\n",
      " 47  146M   47 69.6M    0     0  11.3M      0  0:00:12  0:00:06  0:00:06 14.0M\n",
      " 59  146M   59 86.6M    0     0  12.1M      0  0:00:12  0:00:07  0:00:05 16.8M\n",
      " 70  146M   70  103M    0     0  12.7M      0  0:00:11  0:00:08  0:00:03 16.9M\n",
      " 82  146M   82  120M    0     0  13.1M      0  0:00:11  0:00:09  0:00:02 16.9M\n",
      " 93  146M   93  137M    0     0  13.5M      0  0:00:10  0:00:10 --:--:-- 16.9M\n",
      "100  146M  100  146M    0     0  13.6M      0  0:00:10  0:00:10 --:--:-- 16.7M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  146M    0 14323    0     0  31977      0  1:20:16 --:--:--  1:20:16 33078\n",
      "  8  146M    8 12.6M    0     0   9.9M      0  0:00:14  0:00:01  0:00:13 10.0M\n",
      " 20  146M   20 29.4M    0     0  12.9M      0  0:00:11  0:00:02  0:00:09 13.0M\n",
      " 31  146M   31 46.3M    0     0  14.1M      0  0:00:10  0:00:03  0:00:07 14.2M\n",
      " 43  146M   43 63.4M    0     0  14.8M      0  0:00:09  0:00:04  0:00:05 14.9M\n",
      " 54  146M   54 80.4M    0     0  15.2M      0  0:00:09  0:00:05  0:00:04 16.6M\n",
      " 66  146M   66 97.4M    0     0  15.5M      0  0:00:09  0:00:06  0:00:03 16.9M\n",
      " 77  146M   77  114M    0     0  15.7M      0  0:00:09  0:00:07  0:00:02 17.0M\n",
      " 89  146M   89  131M    0     0  15.8M      0  0:00:09  0:00:08  0:00:01 17.0M\n",
      "100  146M  100  146M    0     0  16.0M      0  0:00:09  0:00:09 --:--:-- 17.0M\n"
     ]
    }
   ],
   "source": [
    "! mkdir encoder\n",
    "! curl -Lo encoder/infersent1.pkl https://dl.fbaipublicfiles.com/infersent/infersent1.pkl\n",
    "! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ede4c286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VaishuSistas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd38d97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import stuff\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76f2fb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Load model\n",
    "from models import InferSent\n",
    "model_version = 1 # Use model_version = 2 to use FastText\n",
    "MODEL_PATH = \"encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19ba67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = False\n",
    "model = model.cuda() if use_cuda else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8f2709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide Location of the Glove Folder and FastText Folder\n",
    "# W2V_PATH = 'content/GloVe/glove.840B.300d.txt' if model_version == 1 else 'content/fastText/crawl-300d-2M.vec'\n",
    "W2V_PATH = 'C:/Users/VaishuSistas/GloVe/glove.840B.300d.txt' if model_version == 1 else 'C:/Users/VaishuSistas/fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60d91821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings of K most frequent words\n",
    "model.build_vocab_k_words(K=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35a47ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some sentences\n",
    "sentences_1 = df_inferSent[\"Sentence 1\"].tolist()\n",
    "sentences_2 = df_inferSent[\"Sentence 2\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aaf363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VaishuSistas\\NLP_Assignment1\\models.py:209: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sentences = np.array(sentences)[idx_sort]\n"
     ]
    }
   ],
   "source": [
    "embeddings_1 = model.encode(df_inferSent[\"Sentence 1\"].tolist(),True)\n",
    "embeddings_2 = model.encode(df_inferSent[\"Sentence 2\"].tolist(),True)\n",
    "print('No of Sentences encoded : {0}'.format(len(embeddings_1)))\n",
    "#embeddings_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c44ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648151c",
   "metadata": {},
   "outputs": [],
   "source": [
    " def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96970273",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferSentScore = []\n",
    "for i in range(len(embeddings_1)):\n",
    "     inferSentScore.append(cosine(embeddings_1[i], embeddings_2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cc350",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferSentScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9eea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferSentScore_5 = [x* 5 for x in inferSentScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f4338",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferSentScore_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dbd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inferSent.insert( loc=2 , column = \"Raw Score\" , value = inferSentScore)\n",
    "df_inferSent.insert( loc=3 , column = \"Scaled Score\" , value = inferSentScore_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_inferSent['Link 1']\n",
    "del df_inferSent['Link 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db27d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inferSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inferSent['Scaled Score'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "infersent_maxScoreSentences = df_inferSent['Scaled Score'].values == df_inferSent['Scaled Score'].max()\n",
    "df_inferSent.loc[infersent_maxScoreSentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "infersent_minScoreSentences = df_inferSent['Scaled Score'].values == df_inferSent['Scaled Score'].min()\n",
    "df_inferSent.loc[infersent_minScoreSentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3159f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inferSent.to_csv(r'STS2016.input.plagiarism.txt.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2058246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae79744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b8ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
